{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lil_lawnmower/Desktop/Code/graphrag/test_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_core.tools import tool, StructuredTool\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from langchain.agents import Tool\n",
    "import asyncio\n",
    "from typing import Any, Dict, List\n",
    "from graph_search import setup_global_search, setup_local_search, create_search_engine\n",
    "from langchain.callbacks.base import AsyncCallbackHandler, BaseCallbackHandler\n",
    "from langchain_core.outputs import LLMResult\n",
    "from graphrag.query.indexer_adapters import read_indexer_entities, read_indexer_reports\n",
    "from graphrag.query.llm.oai.chat_openai import ChatOpenAI\n",
    "from graphrag.query.llm.oai.typing import OpenaiApiType\n",
    "from graphrag.query.structured_search.global_search.community_context import GlobalCommunityContext\n",
    "from graphrag.query.structured_search.global_search.search import GlobalSearch\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai.chat_models import ChatOpenAI as OAI\n",
    "\n",
    "#Define api_key and llm_model\n",
    "api_key = #PLACEHOLDER\n",
    "llm_model = 'gpt-3.5-turbo'\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "        api_key=api_key,\n",
    "        model=llm_model,\n",
    "        api_type=OpenaiApiType.OpenAI,  # OpenaiApiType.OpenAI or OpenaiApiType.AzureOpenAI\n",
    "        max_retries=20,\n",
    "    )\n",
    "\n",
    "#Formats response to input into LLM chain\n",
    "def format_response(query, graph_rag_answer):\n",
    "    prompt = f\"Given the following question by the user and the database result synthesize an answer:\\n\\n\"\n",
    "    prompt += f\"User Question: {query}\"\n",
    "    prompt += f\"Database Answer: {graph_rag_answer}\"\n",
    "    return prompt\n",
    "\n",
    "#Queries the graph\n",
    "async def query_graph(prompt, clearance_level = 'general'):\n",
    "\n",
    "    input_dir = #Define input dir \n",
    "    #based on clearance_level parameter query the correct file (CHANGE THIS FOR YOUR OWN INDEXES)\n",
    "    #clearance_dir example clearance_lev/output/20287712-105687/artifacts\n",
    "    if clearance_level == 'admin':\n",
    "        clearance_dir = #place holder\n",
    "    \n",
    "    elif clearance_level == 'level_q':\n",
    "        clearance_dir = #placeholder\n",
    "\n",
    "    elif clearance_level == 'general':\n",
    "        clearance_dir = #placeholder\n",
    "\n",
    "        \n",
    "\n",
    "    token_encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    # parquet files generated from indexing pipeline\n",
    "    INPUT_DIR = f\"{input_dir}/{clearance_dir}\"\n",
    "    COMMUNITY_REPORT_TABLE = \"create_final_community_reports\"\n",
    "    ENTITY_TABLE = \"create_final_nodes\"\n",
    "    ENTITY_EMBEDDING_TABLE = \"create_final_entities\"\n",
    "\n",
    "\n",
    "    # community level in the Leiden community hierarchy from which we will load the community reports\n",
    "    # higher value means we use reports from more fine-grained communities (at the cost of higher computation cost)\n",
    "    COMMUNITY_LEVEL = 2\n",
    "\n",
    "    entity_df = pd.read_parquet(f\"{INPUT_DIR}/{ENTITY_TABLE}.parquet\")\n",
    "    report_df = pd.read_parquet(f\"{INPUT_DIR}/{COMMUNITY_REPORT_TABLE}.parquet\")\n",
    "    entity_embedding_df = pd.read_parquet(f\"{INPUT_DIR}/{ENTITY_EMBEDDING_TABLE}.parquet\")\n",
    "\n",
    "    reports = read_indexer_reports(report_df, entity_df, COMMUNITY_LEVEL)\n",
    "    entities = read_indexer_entities(entity_df, entity_embedding_df, COMMUNITY_LEVEL)\n",
    "    report_df.head()\n",
    "\n",
    "    context_builder = GlobalCommunityContext(\n",
    "        community_reports=reports,\n",
    "        entities=entities,  # default to None if you don't want to use community weights for ranking\n",
    "        token_encoder=token_encoder,\n",
    "    )\n",
    "    context_builder_params = {\n",
    "        \"use_community_summary\": False,  # False means using full community reports. True means using community short summaries.\n",
    "        \"shuffle_data\": True,\n",
    "        \"include_community_rank\": True,\n",
    "        \"min_community_rank\": 0,\n",
    "        \"community_rank_name\": \"rank\",\n",
    "        \"include_community_weight\": True,\n",
    "        \"community_weight_name\": \"occurrence weight\",\n",
    "        \"normalize_community_weight\": True,\n",
    "        \"max_tokens\": 12_000,  # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 5000)\n",
    "        \"context_name\": \"Reports\",\n",
    "    }\n",
    "\n",
    "    map_llm_params = {\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 0.0,\n",
    "        \"response_format\": {\"type\": \"json_object\"},\n",
    "    }\n",
    "\n",
    "    reduce_llm_params = {\n",
    "        \"max_tokens\": 2000,  # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 1000-1500)\n",
    "        \"temperature\": 0.0,\n",
    "    }\n",
    "    search_engine = GlobalSearch(\n",
    "        llm=llm,\n",
    "        context_builder=context_builder,\n",
    "        token_encoder=token_encoder,\n",
    "        max_data_tokens=12_000,  # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 5000)\n",
    "        map_llm_params=map_llm_params,\n",
    "        reduce_llm_params=reduce_llm_params,\n",
    "        allow_general_knowledge=False,  # set this to True will add instruction to encourage the LLM to incorporate general knowledge in the response, which may increase hallucinations, but could be useful in some use cases.\n",
    "        json_mode=True,  # set this to False if your LLM model does not support JSON mode.\n",
    "        context_builder_params=context_builder_params,\n",
    "        concurrent_coroutines=32,\n",
    "        response_type=\"multiple paragraphs\",  # free form text describing the response type and format, can be anything, e.g. prioritized list, single paragraph, multiple paragraphs, multiple-page report\n",
    "    )\n",
    "\n",
    "    result = await search_engine.asearch(prompt)\n",
    "    return  result.response\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'where is the main character walking to?'\n",
    "graph_rag_answer = await query_graph(prompt = 'where is the main character walking to?', clearance_level = 'level_q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided in the dataset, the main character is depicted walking from Shin Kobe Station to Sannomiya in Kobe. This journey is not just a physical transit but holds deeper significance within the narrative. It symbolizes emotional connections, transitions, and pivotal moments in the character's development. The repeated emphasis on this specific route suggests that it plays a crucial role in shaping the character's experiences and the overall storyline. Further exploration into how this journey influences the character's development and the broader themes of the narrative could provide valuable insights into the narrative's complexities and character dynamics.\n"
     ]
    }
   ],
   "source": [
    "formatted_prompt = format_response(query, graph_rag_answer)\n",
    "\n",
    "#Define LLM for the langchain\n",
    "llm = OAI(\n",
    "        api_key=api_key,\n",
    "        model=llm_model,\n",
    "    )\n",
    "\n",
    "#Define prompt to give LLM chain\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that synthesizes information and give relevant information back to the user. You are also honest if you don't know the answer to a question. If the question doesn't need specific data and can be answered with general knowledge, you can answer it. If the question needs specific data, you can ask for clarification. If the question is too broad, you can ask for more details. If the question is inappropriate, you can refuse to answer.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#Chain LLM and prompt\n",
    "chain = prompt | llm\n",
    "\n",
    "#Call LLM\n",
    "answer = chain.invoke({\n",
    "    \"input\": formatted_prompt\n",
    "})\n",
    "\n",
    "print(answer.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
